---
description: >-
  Fuzzing web es una técnica de pruebas a menudo automatizada o
  semiautomatizada, que nos permite conocer más direcciones de un servidor web.
---

# Fuzzing Web

Una vez hemos terminado con esa enumeración manual nos toca comenzar con herramientas un poco más automáticas.

### NMAP

```
nmap
```

Nmap también tiene sus scripts para poder realizar un fuzzing web básico

### GOBUSTER Y DIRBUSTER

```
gobuster dir -u http://10.10.10.55/ -w /usr/share/dirb/wordlists/common.txt -t 50
dirb http://10.10.10.55/ /usr/share/dirb/wordlists/common.txt
```

Existen distintos diccionarios e incluso podemos crear los nuestros propios sin ninguna complicación, normalmente en Parrot o Kali los encontramos como:

```
/usr/share/dirb/wordlists/common.txt
/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt
/usr/share/dirbuster/wordlists/directory-list-lowercase-2.3-small.txt
```

Además podemos seleccionar distintas terminaciones que queremos usar como .txt, .php, .html...

```
dirb http://10.10.10.55/ /usr/share/dirb/wordlists/common.txt -X .php,.old,.bak,.html,.htm,.txt,""
gobuster dir -u http://10.10.120.55/ -w /usr/share/dirb/wordlists/common.txt -x php,old,bak,html,htm,txt,"" -t 50
```

### ¿NUESTRAS PROPIAS HERRAMIENTAS?

Tenemos que preguntarnos ¿Qué hacen estas herramientas? Pues realmente envían una petición a la página web y en función de cómo responda determinamos si existe o no.

Cuando una web responde con un 404 quiere decir que no existe dicha dirección, por lo que podríamos descartarla. Por esta razón podríamos usar por ejemplo la herramienta curl y cuando exista un 404 desechar dicha url.

#### Algunos códigos de estado

* 200 - OK
* 301 - Movido permanentemente
* 403 - Petición correcta pero no puedes acceder
* 404 - No encontrado

## Wayback Machine

Se pueden obtener distintos _endpoints_ directamente con otras herramientas cómo Wayback Machine.

```bash
curl -s "https://web.archive.org/cdx/search/cdx?url=dominio.es*&output=txt" -o urls
awk '{print $3}' urls | sort | uniq > urls
```

Tengo un pequeño script en Bash que además comprueba si dichos _endpoints_ siguen activos devolviendo el código de estado.

```bash
#!/bin/bash

dominio=$1
archivo=$2

url="https://web.archive.org/cdx/search/cdx?url=$dominio*&output=txt"
echo -e "Hacemos el curl a $url\n\n"

curl $url -o urls1

echo -e "Hacemos el awk\n\n"
awk '{print $3}' urls1 | sort | uniq > urls2

echo -e "Borramos urls1\n\n"
rm urls1

echo -e "Iteramos\n\n"
while read i
        do response=$(curl --write-out '%{http_code}' --silent --output /dev/null $i)
        echo -e "$i \t $response" >> $archivo
        echo -e "$i \t $response"
done < urls2

echo -e "Borramos urls2\n\n"
rm urls2
echo "Se acabo"bash
```

### Practicar

* Máquina "[crocodile](../HTB/htb/crocodile.md)" de HTB \[Acceso 21/10/2021]
* Máquina "[Preignition](../HTB/htb/preignition.md)" de HTB \[Acceso 10/12/2021]
* Máquina "[Ignition](../HTB/htb/ignition.md)" de HTB \[Acceso 10/12/2021]
* Máquina "[Admirer](https://www.youtube.com/watch?v=hoQ6-wUldHg)" de HTB \[Acceso 11/02/2022]

> \*Disclaimer: Este manual está realizado en base a lo que voy aprendiendo, si parte de la información que comparto no es correcta estaré encantado de que te pongas en contacto conmigo para hacérmelo saber y poder modificarlo.
